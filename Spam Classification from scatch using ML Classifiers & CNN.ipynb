{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"F:\\\\Analytics Vidhya Al & ML\\\\NLP\\\\Spam Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"spam.csv\",encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ï»¿label                                               text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play cricket today'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punctuation=string.punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopword_list=stopwords.words(\"english\")\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()\n",
    "def _clean(text):\n",
    "    \n",
    "    cleaned_text=text.lower()\n",
    "    cleaned_text=\"\".join(c for c in cleaned_text if c not in punctuation)\n",
    "    \n",
    "    words= cleaned_text.split()\n",
    "    words=[w for w in words if w not in stopword_list]\n",
    "    \n",
    "    words=[lem.lemmatize(words,\"v\") for words in words]\n",
    "    words=[lem.lemmatize(words,\"n\") for words in words]\n",
    "    \n",
    "    cleaned_text=\" \".join(words)\n",
    "    \n",
    "    \n",
    "    return cleaned_text\n",
    "_clean(\"I will be playing Cricket Today !!!@#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cleaned\"]=data['text'].apply(_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿label</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ï»¿label                                               text  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...   \n",
       "1      ham                      Ok lar... Joking wif u oni...   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      ham  U dun say so early hor... U c already then say...   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                             cleaned  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                              ok lar joke wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4           nah dont think go usf live around though  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Feature Engineering \n",
    "\n",
    "#meta feature : some of the attributes or counts that are associated with the text data\n",
    "\n",
    "data['word_count']=data['text'].apply(lambda x : len(x.split()))\n",
    "data['word_count_cleaned']=data['cleaned'].apply(lambda x : len(x.split()))\n",
    "data['char_count']=data['text'].apply(lambda x : len(x))\n",
    "data['char_count_without_spaces']=data['text'].apply(lambda x: len(x.replace(\" \",\"\")))\n",
    "data['num_digit']=data['text'].apply(lambda x : sum([1 if w.isdigit else 0 for w in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿label</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_cleaned</th>\n",
       "      <th>char_count</th>\n",
       "      <th>char_count_without_spaces</th>\n",
       "      <th>num_digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>111</td>\n",
       "      <td>92</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>155</td>\n",
       "      <td>128</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>39</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf live around though</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ï»¿label                                               text  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...   \n",
       "1      ham                      Ok lar... Joking wif u oni...   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      ham  U dun say so early hor... U c already then say...   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                             cleaned  word_count  \\\n",
       "0  go jurong point crazy available bugis n great ...          20   \n",
       "1                              ok lar joke wif u oni           6   \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...          28   \n",
       "3                u dun say early hor u c already say          11   \n",
       "4           nah dont think go usf live around though          13   \n",
       "\n",
       "   word_count_cleaned  char_count  char_count_without_spaces  num_digit  \n",
       "0                  16         111                         92         20  \n",
       "1                   6          29                         24          6  \n",
       "2                  23         155                        128         28  \n",
       "3                   9          49                         39         11  \n",
       "4                   8          61                         49         13  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other type of features can be NLP features \n",
    "\n",
    "pos_dic={\"noun\":['NNP','NN','NNS','NNPS'],'verb':['VBZ','VB','VBD','VBN']}\n",
    "#defining the function for the count of words of the POS\n",
    "import nltk\n",
    "def pos_check(text,family):\n",
    "    tags=nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    count=0\n",
    "    for tag in tags: #it will return list of tags and corresponding POStag\n",
    "        tag=tag[1] # to obtain the proper pos tag we define tag[1]\n",
    "        if tag in pos_dic[family]: # if the pos is present in the particular family then increase the count by 1\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "#pos_check(\"They are playing the ground\",\"noun\")\n",
    "data['noun_count']=data['text'].apply(lambda x: pos_check(x,\"noun\"))\n",
    "data['verb_count']=data['text'].apply(lambda x: pos_check(x,\"verb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿label</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_count_cleaned</th>\n",
       "      <th>char_count</th>\n",
       "      <th>char_count_without_spaces</th>\n",
       "      <th>num_digit</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>111</td>\n",
       "      <td>92</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>155</td>\n",
       "      <td>128</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>39</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah dont think go usf live around though</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ï»¿label                                               text  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...   \n",
       "1      ham                      Ok lar... Joking wif u oni...   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      ham  U dun say so early hor... U c already then say...   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                             cleaned  word_count  \\\n",
       "0  go jurong point crazy available bugis n great ...          20   \n",
       "1                              ok lar joke wif u oni           6   \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...          28   \n",
       "3                u dun say early hor u c already say          11   \n",
       "4           nah dont think go usf live around though          13   \n",
       "\n",
       "   word_count_cleaned  char_count  char_count_without_spaces  num_digit  \\\n",
       "0                  16         111                         92         20   \n",
       "1                   6          29                         24          6   \n",
       "2                  23         155                        128         28   \n",
       "3                   9          49                         39         11   \n",
       "4                   8          61                         49         13   \n",
       "\n",
       "   noun_count  verb_count  \n",
       "0          10           1  \n",
       "1           4           0  \n",
       "2          13           3  \n",
       "3           3           0  \n",
       "4           1           4  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of the ideas to use vectors are count as the feature like count of every word in the document or in a corpus\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#countVectorizer\n",
    "\n",
    "cvz=CountVectorizer()\n",
    "cvz.fit(data['cleaned'].values)\n",
    "count_vectors=cvz.transform(data['cleaned'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tfidf=TfidfVectorizer(max_features=500)\n",
    "word_tfidf.fit(data['cleaned'].values)\n",
    "word_vectors=word_tfidf.transform(data['cleaned'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.581166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>5.936809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>5.754488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>5.888019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150p</th>\n",
       "      <td>6.406813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yet</th>\n",
       "      <td>5.713666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yo</th>\n",
       "      <td>6.099328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youre</th>\n",
       "      <td>5.655397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yr</th>\n",
       "      <td>6.581166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yup</th>\n",
       "      <td>5.841499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_tfidf\n",
       "10       6.581166\n",
       "100      5.936809\n",
       "1000     5.754488\n",
       "150      5.888019\n",
       "150p     6.406813\n",
       "...           ...\n",
       "yet      5.713666\n",
       "yo       6.099328\n",
       "youre    5.655397\n",
       "yr       6.581166\n",
       "yup      5.841499\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf=dict(zip(word_tfidf.get_feature_names(),word_tfidf.idf_))\n",
    "tfidf_df=pd.DataFrame(columns=['word_tfidf']).from_dict(tfidf,orient='index')\n",
    "tfidf_df.columns=['word_tfidf']\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack,csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ï»¿label', 'text', 'cleaned', 'word_count', 'word_count_cleaned',\n",
       "       'char_count', 'char_count_without_spaces', 'num_digit', 'noun_count',\n",
       "       'verb_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features=['word_count', 'word_count_cleaned',\n",
    "       'char_count', 'char_count_without_spaces', 'num_digit', 'noun_count',\n",
    "       'verb_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set=data[meta_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=hstack([word_vectors,csr_matrix(feature_set)],\"csr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x507 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 65862 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "target=data['ï»¿label'].values\n",
    "target=LabelEncoder().fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x,valid_x,train_y,valid_y=train_test_split(train,target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4179, 507), (1393, 507))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9655419956927495"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=naive_bayes.MultinomialNB()\n",
    "model.fit(train_x,train_y)\n",
    "preds=model.predict(valid_x)\n",
    "accuracy_score(preds,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saikrizna\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9511844938980617"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LogisticRegression()\n",
    "model.fit(train_x,train_y)\n",
    "preds=model.predict(valid_x)\n",
    "accuracy_score(preds,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210337401292176"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=svm.SVC()\n",
    "model.fit(train_x,train_y)\n",
    "preds=model.predict(valid_x)\n",
    "accuracy_score(preds,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9813352476669059"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=ensemble.ExtraTreesClassifier()\n",
    "model.fit(train_x,train_y)\n",
    "preds=model.predict(valid_x)\n",
    "accuracy_score(preds,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", -0.0282 -0.0557 -0.0451 -0.0434 0.0712 -0.0855 -0.1085 -0.0561 -0.4523 -0.0202 0.0975 0.1047 0.1962 -0.0693 0.0213 -0.0235 0.1336 -0.0420 -0.0564 -0.0798 0.0424 -0.0409 -0.0536 -0.0252 0.0135 0.0064 0.1235 0.0461 0.0120 -0.0372 0.0650 0.0041 -0.1074 -0.0263 0.1133 -0.0029 0.0671 0.1065 0.0234 -0.0160 0.0070 0.4355 -0.0752 -0.4328 0.0457 0.0604 -0.0740 -0.0055 -0.0089 -0.2926 -0.0545 -0.1519 0.0990 -0.0193 -0.0050 0.0511 0.0404 0.1023 -0.0128 0.0488 -0.1567 -0.0759 -0.0190 0.1442 0.0047 -0.0186 0.0140 -0.0385 -0.0853 0.1572 0.1770 0.0084 -0.0250 -0.1145 -0.0663 -0.1244 -0.3977 -0.0124 -0.4586 -0.0220 0.5746 0.0218 -0.0754 0.0099 0.0397 -0.0154 0.0424 -0.0150 -0.0016 0.0305 0.0101 0.2266 0.1394 0.0189 0.0069 0.0394 0.0355 -0.0111 -0.0687 -0.0078 0.0224 0.0817 -0.1949 0.0001 0.4047 -0.0237 -0.0656 -0.0684 0.0233 0.0438 0.1203 -0.0276 0.0416 0.0114 -0.4529 0.1538 0.1323 -0.0186 -0.0914 -0.0312 0.1051 0.0212 0.0798 -0.0104 -0.0206 -0.0025 0.0043 -0.0378 0.2689 0.0747 -0.0418 -0.0048 -0.0387 0.0432 0.1704 0.0614 0.0905 -0.0436 -0.0141 -0.0315 0.0276 0.0151 -0.0103 -0.0266 -0.0512 -0.0408 -0.0651 0.0662 -0.0936 0.1371 0.0458 -0.1366 -0.0075 -0.0104 -0.0732 0.1205 0.1035 0.0106 -0.0317 -0.0316 0.6639 -0.0022 -0.1343 0.0144 -0.0338 0.0034 -0.0429 -0.0821 0.0037 0.1029 -0.0204 -0.0269 0.0052 -0.1034 0.1068 0.0121 0.0980 -0.0458 0.0199 -0.0132 0.1936 -0.0213 0.0209 -0.0025 0.0416 -0.0337 0.0516 -0.1014 0.0203 0.0198 -0.0305 -0.0313 0.0543 -0.0106 0.1441 -0.0178 -0.0627 0.0475 0.0352 -0.0254 -0.0949 0.0401 0.0317 0.0055 -0.0536 0.0191 -0.0511 -0.0409 -0.0030 0.1582 0.0108 0.5237 0.0436 0.0306 -0.0392 0.0177 0.0069 0.0605 0.1206 -0.0216 -0.0633 -0.2965 0.0521 -0.0150 -0.2207 -0.0642 -0.0906 -0.0121 0.0569 0.0944 -0.0652 -0.0108 -0.0477 0.0023 0.0077 -0.1547 0.0463 0.0698 -0.0376 -0.0291 0.0033 -0.0102 -0.0743 0.0085 0.0805 -0.0291 -0.0674 -0.0586 -0.0653 0.0283 -0.0255 0.0869 -0.0868 0.0090 0.3245 -0.0573 -0.0289 0.0470 -0.0117 0.0174 0.0132 -0.0226 -0.0664 0.0188 0.0263 0.0111 -0.0049 -0.0656 0.0295 0.0435 0.0290 0.1163 0.0448 -0.1139 -0.0553 -0.0528 0.1745 -0.0146 -0.1308 -0.0607 -0.0134 0.0781 0.0378 0.0228 -0.0728 -0.0059 0.0158 -0.0141 -0.0002 0.0193 -0.0148 -0.0463 0.0444 0.3034 0.1020 -0.0871 0.0317 -0.0370 -0.0725 -0.0042\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings_index={}\n",
    "for i , line in enumerate(open('pretrained.vec',encoding=\"utf8\")): #opening file & iterating one by one \n",
    "    if i ==0:\n",
    "        continue\n",
    "    value = line.split()#splitting the each line \n",
    "    print(line)\n",
    "    break\n",
    "    embeddings_index[value[0]]=np.array(value[1:],dtype='float32') # all the lines that contains first element as word & corresponding word vector\n",
    "                                                   # and converting them to numpy array instead of raw strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Saikrizna\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Saikrizna\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Saikrizna\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Saikrizna\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Saikrizna\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Saikrizna\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# converting text data into word embedding \n",
    "from keras.preprocessing import text, sequence \n",
    "\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(data['text'])\n",
    "word_index=token.word_index\n",
    "\n",
    "#Next step is to convert the text into sequence of tokens & also pad them to make sure equal length of vectors are obtained\n",
    "train_x,valid_x,train_y,valid_y=train_test_split(data['text'],target)\n",
    "\n",
    "\n",
    "train_x=sequence.pad_sequences(token.texts_to_sequences(train_x),maxlen=70)\n",
    "valid_x=sequence.pad_sequences(token.texts_to_sequences(valid_x),maxlen=70)\n",
    "\n",
    "embedding_matrix= np.zeros((len(word_index)+1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector=embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier,feature_vector_train,label,feature_vector_val,valid_y):\n",
    "    classifier.fit(feature_vector_train,label)\n",
    "    predictions=classifier.predict(feature_vector_val)\n",
    "    predictions=predictions.argmax(axis=-1)\n",
    "    return accuracy_score(predictions,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models,optimizers\n",
    "\n",
    "def create_cnn():\n",
    "    \n",
    "    input_layer =layers.Input((70,))\n",
    "    \n",
    "    embedding_layer=layers.Embedding(len(word_index)+1,300,weights=[embedding_matrix],trainable=False)(input_layer)\n",
    "    \n",
    "    conv_layer = layers.Convolution1D(100,3,activation='relu')(embedding_layer)\n",
    "    \n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "    \n",
    "    output_layer = layers.Dense(50,activation='relu')(pooling_layer)\n",
    "    output_layer = layers.Dropout(0.25)(output_layer)\n",
    "    output_layer = layers.Dense(1,activation='sigmoid')(output_layer)\n",
    "    \n",
    "    model=models.Model(inputs=input_layer,output=output_layer)\n",
    "    model.compile(optimizer=optimizers.Adam(),loss='binary_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saikrizna\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4179/4179 [==============================] - 5s 1ms/step - loss: 0.6711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8815506101938263"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier =create_cnn()\n",
    "train_model(classifier,train_x,train_y,valid_x,valid_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
